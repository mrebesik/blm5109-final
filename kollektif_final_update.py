# -*- coding: utf-8 -*-
"""kollektif_final_update.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h4E5kAG_RUSBjOK8IM8ckIw1Xafkbk6A
"""

!pip -q uninstall -y transformers sentence-transformers accelerate tokenizers safetensors
!pip -q install --no-cache-dir -U \
  "transformers==4.41.2" \
  "sentence-transformers==2.7.0" \
  "accelerate==0.30.1" \
  "tokenizers==0.19.1" \
  "safetensors>=0.4.3"

import transformers, sentence_transformers
print("transformers:", transformers.__version__)
print("sentence-transformers:", sentence_transformers.__version__)

import os, gc, json, random, traceback
import torch
from torch.utils.data import DataLoader
import pandas as pd

from datasets import load_dataset
from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation
from safetensors.torch import load_file, save_file

from google.colab import drive
drive.mount('/content/drive')

# ----------------------------
# CONFIG
# ----------------------------
PROJE_PATH = "/content/drive/MyDrive/Contrastive_Ensembles_Proje_Final_Updated"
os.makedirs(PROJE_PATH, exist_ok=True)

MODEL_NAME = "ytu-ce-cosmos/turkish-e5-large"

SEEDS = [42, 123, 999]
SUBSET_SIZE = 2500
EPOCHS = 3
LR = 2e-5

TRAIN_BATCH = 4
EVAL_QA_COUNT = 500
EVAL_BATCH = 4

task_instruct = "Verilen TÃ¼rkÃ§e arama sorgusu ile ilgili pasajlarÄ± getir."

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

torch.manual_seed(0)
random.seed(0)

# ----------------------------
# ----------------------------
def clear_mem():
    torch.cuda.empty_cache()
    gc.collect()

def build_model(model_name_or_path, device="cuda"):
    m = SentenceTransformer(model_name_or_path, device=device)
    try:
        first = m._first_module()
        if hasattr(first, "auto_model") and hasattr(first.auto_model, "gradient_checkpointing_enable"):
            first.auto_model.gradient_checkpointing_enable()
            if hasattr(first.auto_model, "config"):
                first.auto_model.config.use_cache = False
            print("âœ… gradient checkpointing enabled")
    except Exception as e:
        print(" checkpointing aÃ§Ä±lamadÄ±:", e)
    return m

def is_valid_pair(q, p):
    if (q is None) or (p is None): return False
    if len(str(q)) < 5: return False
    if len(str(p)) < 20: return False
    return True

def make_query_text(q):
    return f"Instruct: {task_instruct}\nQuery: {q}"

def build_examples_from_indices(ds, indices):
    ex = []
    kept = []
    for i in indices:
        row = ds[i]
        q = row.get("question", None)
        p = row.get("context", None)
        if not is_valid_pair(q, p):
            continue
        ex.append(InputExample(texts=[make_query_text(q), p]))  # (q, pos) only
        kept.append(i)
    return ex, kept

def sample_subset_indices(ds, seed, subset_size):
    N = len(ds)
    perm = list(range(N))
    rng = random.Random(seed)
    rng.shuffle(perm)

    chosen = []
    for idx in perm:
        row = ds[idx]
        q = row.get("question", None)
        p = row.get("context", None)
        if is_valid_pair(q, p):
            chosen.append(idx)
            if len(chosen) >= subset_size:
                break

    if len(chosen) < subset_size:
        raise RuntimeError(f"Seed {seed}: Only {len(chosen)} valid samples found, need {subset_size}.")
    return chosen

def train_with_oom_fallback(model, train_examples, out_path, epochs, lr, prefer_batch):
    for bs in [prefer_batch, 2, 1]:
        try:
            print(f"ðŸš€ Training -> {out_path} | batch={bs} | epochs={epochs} | lr={lr}")
            train_loader = DataLoader(train_examples, shuffle=True, batch_size=bs)
            train_loss = losses.MultipleNegativesRankingLoss(model=model)

            model.fit(
                train_objectives=[(train_loader, train_loss)],
                epochs=epochs,
                warmup_steps=int(len(train_loader) * 0.1),
                output_path=out_path,
                optimizer_params={"lr": lr},
                use_amp=True,
                show_progress_bar=True
            )
            return bs
        except RuntimeError as e:
            msg = str(e).lower()
            if "out of memory" in msg or "cuda out of memory" in msg:
                print(f"âš ï¸ OOM with batch={bs}. Retrying with smaller batch...")
                clear_mem()
                continue
            raise
    raise RuntimeError("OOM: Even batch=1 failed.")

def load_state_dict_any(path):
    st_path = os.path.join(path, "model.safetensors")
    bin_path = os.path.join(path, "pytorch_model.bin")
    if os.path.exists(st_path):
        return load_file(st_path)
    if os.path.exists(bin_path):
        return torch.load(bin_path, map_location="cpu")
    raise FileNotFoundError(f"No weights found in {path}")

def merge_models_average(model_paths, out_path, base_model_name):
    """
    Model soup: arithmetic mean of weights.
    """
    print("\n Merging models (weight average)...")
    merged = None
    for i, p in enumerate(model_paths):
        sd = load_state_dict_any(p)
        if merged is None:
            merged = {k: v.float().clone() for k, v in sd.items()}
        else:
            for k in merged:
                merged[k] += sd[k].float()

    K = len(model_paths)
    for k in merged:
        merged[k] /= K

    final_model = SentenceTransformer(base_model_name)
    final_model[0].auto_model.load_state_dict(merged, strict=False)
    final_model.save(out_path, safe_serialization=True)
    print(f"âœ… MERGED saved: {out_path}")

# ----------------------------
# 1) LOAD DATA
# ----------------------------
print(" Loading dataset: boun-tabilab/TQuad-2 ...")
train_ds = load_dataset("boun-tabilab/TQuad-2", split="train")
val_ds   = load_dataset("boun-tabilab/TQuad-2", split="validation")
print("Train size:", len(train_ds), "| Val size:", len(val_ds))

# ----------------------------
# 2) BUILD EVALUATOR (500 validation queries)
# ----------------------------
print("\nðŸ§ª Building evaluator (500 val queries)...")
val_sub = val_ds.select(range(min(EVAL_QA_COUNT, len(val_ds))))

queries, corpus, relevant_docs = {}, {}, {}
for row in val_sub:
    q_id = str(row.get("id", hash(row["question"])))
    doc_id = q_id + "_doc"
    queries[q_id] = make_query_text(row["question"])
    corpus[doc_id] = row["context"]
    relevant_docs[q_id] = {doc_id}

evaluator = evaluation.InformationRetrievalEvaluator(
    queries=queries,
    corpus=corpus,
    relevant_docs=relevant_docs,
    name="TQuad_Benchmark",
    show_progress_bar=False,
    mrr_at_k=[10],
    accuracy_at_k=[1, 5],
    batch_size=EVAL_BATCH
)

def eval_model(name, path_or_name):
    clear_mem()
    print(f"\nðŸ” Evaluating: {name} -> {path_or_name}")
    m = SentenceTransformer(path_or_name, device="cuda")
    with torch.no_grad():
        s = evaluator(m)
    return s

def pick_metrics(scores):
    return {
        "MRR@10": scores["TQuad_Benchmark_cosine_mrr@10"],
        "Acc@1":  scores["TQuad_Benchmark_cosine_accuracy@1"],
        "Acc@5":  scores["TQuad_Benchmark_cosine_accuracy@5"],
        "nDCG@10": scores["TQuad_Benchmark_cosine_ndcg@10"],
        "MAP@100": scores["TQuad_Benchmark_cosine_map@100"],
    }

# ----------------------------
# 3) SAMPLE 3 TRUE SUBSETS + TRAIN 3 MODELS
# ----------------------------
print("\nðŸ§© Sampling 3 subsets (true subsets, seed affects chosen examples)...")
subset_indices = {}
seed_train_paths = []
seed_used_batches = {}

for seed in SEEDS:
    idx = sample_subset_indices(train_ds, seed, SUBSET_SIZE)
    subset_indices[seed] = idx

subset_path = os.path.join(PROJE_PATH, "subset_indices.json")
with open(subset_path, "w") as f:
    json.dump(subset_indices, f)
print("âœ… Saved subset indices:", subset_path)
print("Subset sizes:", {s: len(subset_indices[s]) for s in SEEDS})

for seed in SEEDS:
    print(f"\n==============================")
    print(f" TRAINING SEED MODEL: {seed}")
    print(f"==============================")

    # build examples for that subset (with filtering)
    train_ex, kept = build_examples_from_indices(train_ds, subset_indices[seed])
    print(f"Subset {seed}: requested={len(subset_indices[seed])}, kept(valid)={len(kept)}")

    save_path = os.path.join(PROJE_PATH, f"model_seed_{seed}")
    seed_train_paths.append(save_path)

    clear_mem()
    model = build_model(MODEL_NAME, device="cuda")

    used_bs = train_with_oom_fallback(
        model=model,
        train_examples=train_ex,
        out_path=save_path,
        epochs=EPOCHS,
        lr=LR,
        prefer_batch=TRAIN_BATCH
    )
    seed_used_batches[seed] = used_bs
    print(f"âœ… Seed {seed} trained. Used batch={used_bs}. Saved: {save_path}")

    del model
    clear_mem()

# ----------------------------
# 4) MERGE (MODEL SOUP)
# ----------------------------
MERGED_PATH = os.path.join(PROJE_PATH, "cosmos_merged_final")
merge_models_average(seed_train_paths, MERGED_PATH, MODEL_NAME)

# ----------------------------
# 5) UNION MODEL (union of the 3 subsets)
# ----------------------------
print("\n Building UNION indices and training a single union model...")
union_idx = sorted(set().union(*[set(subset_indices[s]) for s in SEEDS]))
union_ex, union_kept = build_examples_from_indices(train_ds, union_idx)
print(f"Union indices size={len(union_idx)} | kept(valid)={len(union_kept)}")

UNION_PATH = os.path.join(PROJE_PATH, "model_union_3subsets")
clear_mem()
union_model = build_model(MODEL_NAME, device="cuda")
union_used_bs = train_with_oom_fallback(
    model=union_model,
    train_examples=union_ex,
    out_path=UNION_PATH,
    epochs=EPOCHS,
    lr=LR,
    prefer_batch=TRAIN_BATCH
)
print(f" Union trained. Used batch={union_used_bs}. Saved: {UNION_PATH}")
del union_model
clear_mem()

# 6) EVALUATE ALL
print("\n Evaluating BASE + SEEDS + MERGED + UNION (same evaluator)...")

results = []

# BASE
scores_base = eval_model("BASE (cosmos)", MODEL_NAME)
results.append({"Model": "BASE (cosmos)", **pick_metrics(scores_base)})

# SEED models
for seed in SEEDS:
    p = os.path.join(PROJE_PATH, f"model_seed_{seed}")
    sc = eval_model(f"Model (seed {seed})", p)
    results.append({"Model": f"Model (seed {seed})", **pick_metrics(sc)})

# MERGED
scores_merged = eval_model("MERGED ENSEMBLE", MERGED_PATH)
results.append({"Model": "MERGED ENSEMBLE", **pick_metrics(scores_merged)})

# UNION
scores_union = eval_model("UNION (3 subsets)", UNION_PATH)
results.append({"Model": "UNION (3 subsets)", **pick_metrics(scores_union)})

df = pd.DataFrame(results).sort_values("MRR@10", ascending=False)

print("\nâœ… RESULTS (same evaluator):")
print(df.to_string(index=False))

# Save results
csv_path = os.path.join(PROJE_PATH, "results_same_evaluator.csv")
json_path = os.path.join(PROJE_PATH, "results_same_evaluator.json")
df.to_csv(csv_path, index=False)
with open(json_path, "w") as f:
    json.dump(results, f, indent=2)

print("\n Saved:")
print(" -", csv_path)
print(" -", json_path)

print(" DONE.")

!pip -q uninstall -y transformers sentence-transformers accelerate tokenizers safetensors
!pip -q install --no-cache-dir -U \
  "transformers==4.41.2" \
  "sentence-transformers==2.7.0" \
  "accelerate==0.30.1" \
  "tokenizers==0.19.1" \
  "safetensors>=0.4.3"

import transformers, sentence_transformers
print("transformers:", transformers.__version__)
print("sentence-transformers:", sentence_transformers.__version__)

import os, gc, json
import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, InputExample, losses

from google.colab import drive
drive.mount('/content/drive')

# ---- CONFIG ----
PROJE_PATH = "/content/drive/MyDrive/Contrastive_Ensembles_Proje_Final_Updated"
MODEL_NAME = "ytu-ce-cosmos/turkish-e5-large"
SEEDS = [42, 123, 999]

EPOCHS = 3
LR = 2e-5
TRAIN_BATCH = 4
task_instruct = "Verilen TÃ¼rkÃ§e arama sorgusu ile ilgili pasajlarÄ± getir."

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

def clear_mem():
    torch.cuda.empty_cache()
    gc.collect()

def exists_model(path):
    return os.path.exists(os.path.join(path, "modules.json")) and (
        os.path.exists(os.path.join(path, "model.safetensors")) or
        os.path.exists(os.path.join(path, "pytorch_model.bin"))
    )

def is_valid_pair(q, p):
    return (q is not None) and (p is not None) and len(str(q)) >= 5 and len(str(p)) >= 20

def make_query_text(q):
    return f"Instruct: {task_instruct}\nQuery: {q}"

def build_examples_from_indices(ds, indices):
    ex, kept = [], []
    for i in indices:
        row = ds[i]
        q = row.get("question")
        p = row.get("context")
        if not is_valid_pair(q, p):
            continue
        ex.append(InputExample(texts=[make_query_text(q), p]))
        kept.append(i)
    return ex, kept

def build_model(model_name_or_path, device="cuda"):
    m = SentenceTransformer(model_name_or_path, device=device)
    try:
        first = m._first_module()
        if hasattr(first, "auto_model") and hasattr(first.auto_model, "gradient_checkpointing_enable"):
            first.auto_model.gradient_checkpointing_enable()
            if hasattr(first.auto_model, "config"):
                first.auto_model.config.use_cache = False
            print("âœ… gradient checkpointing enabled")
    except Exception as e:
        print("âš ï¸ checkpointing aÃ§Ä±lamadÄ±:", e)
    return m

def train_with_oom_fallback(model, train_examples, out_path, epochs, lr, prefer_batch):
    for bs in [prefer_batch, 2, 1]:
        try:
            print(f"ðŸš€ Training UNION -> {out_path} | batch={bs} | epochs={epochs} | lr={lr}")
            dl = DataLoader(train_examples, shuffle=True, batch_size=bs)
            loss = losses.MultipleNegativesRankingLoss(model=model)
            model.fit(
                train_objectives=[(dl, loss)],
                epochs=epochs,
                warmup_steps=int(len(dl) * 0.1),
                output_path=out_path,
                optimizer_params={"lr": lr},
                use_amp=True,
                show_progress_bar=True
            )
            return bs
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"âš ï¸ OOM with batch={bs}, retry smaller...")
                clear_mem()
                continue
            raise
    raise RuntimeError("OOM even with batch=1")

print("ðŸ“¦ Loading train split...")
train_ds = load_dataset("boun-tabilab/TQuad-2", split="train")

subset_path = os.path.join(PROJE_PATH, "subset_indices.json")
with open(subset_path, "r") as f:
    subset_indices = json.load(f)
subset_indices = {int(k): v for k, v in subset_indices.items()}

union_idx = sorted(set().union(*[set(subset_indices[s]) for s in SEEDS]))
union_ex, kept = build_examples_from_indices(train_ds, union_idx)
print(f"Union indices size={len(union_idx)} | kept(valid)={len(kept)}")

UNION_PATH = os.path.join(PROJE_PATH, "model_union_3subsets")

if exists_model(UNION_PATH):
    print("âœ… UNION already exists, skipping:", UNION_PATH)
else:
    clear_mem()
    model = build_model(MODEL_NAME, device="cuda")
    used_bs = train_with_oom_fallback(model, union_ex, UNION_PATH, EPOCHS, LR, TRAIN_BATCH)
    print(f"âœ… UNION DONE. Used batch={used_bs}. Saved:", UNION_PATH)
    del model
    clear_mem()

!pip -q uninstall -y sentence-transformers transformers accelerate tokenizers
!pip -q cache purge

!pip -q install --no-cache-dir -U \
  "transformers==4.47.1" \
  "sentence-transformers==3.1.1" \
  "accelerate>=0.26.0" \
  "tokenizers>=0.20.0" \
  "safetensors>=0.4.3"

import transformers, sentence_transformers
print("transformers:", transformers.__version__)
print("sentence-transformers:", sentence_transformers.__version__)

import os, gc
import torch, pandas as pd
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, evaluation

PROJE_PATH = "/content/drive/MyDrive/Contrastive_Ensembles_Proje_Final_Updated"
MODEL_NAME = "ytu-ce-cosmos/turkish-e5-large"
task_instruct = "Verilen TÃ¼rkÃ§e arama sorgusu ile ilgili pasajlarÄ± getir."

EVAL_QA_COUNT = 500
EVAL_BATCH = 4

def clear_mem():
    torch.cuda.empty_cache(); gc.collect()

def make_query_text(q):
    return f"Instruct: {task_instruct}\nQuery: {q}"

# --- Build evaluator once ---
val_ds = load_dataset("boun-tabilab/TQuad-2", split="validation").select(range(EVAL_QA_COUNT))
queries, corpus, relevant_docs = {}, {}, {}
for row in val_ds:
    q_id = str(row.get("id", hash(row["question"])))
    doc_id = q_id + "_doc"
    queries[q_id] = make_query_text(row["question"])
    corpus[doc_id] = row["context"]
    relevant_docs[q_id] = {doc_id}

evaluator = evaluation.InformationRetrievalEvaluator(
    queries=queries, corpus=corpus, relevant_docs=relevant_docs,
    name="TQuad_Benchmark", show_progress_bar=False,
    mrr_at_k=[10], accuracy_at_k=[1, 5], batch_size=EVAL_BATCH
)

def ensure_metrics_dict(out, name="TQuad_Benchmark"):
    """
    evaluator(model) dÃ¶nÃ¼ÅŸÃ¼ dict deÄŸilse (float vs.) yakala ve anlamlÄ± hata ver.
    """
    if isinstance(out, dict):
        return out
    raise TypeError(f"Evaluator output is not a dict (got {type(out)}: {out}). "
                    f"This usually indicates version mismatch. Please ensure sentence-transformers>=3.x.")

def pick(scores):
    # ST 3.x key formatÄ± bu ÅŸekilde geliyor (senin eski Ã§Ä±ktÄ±n gibi)
    return {
        "MRR@10": scores[f"{evaluator.name}_cosine_mrr@10"],
        "Acc@1":  scores[f"{evaluator.name}_cosine_accuracy@1"],
        "Acc@5":  scores[f"{evaluator.name}_cosine_accuracy@5"],
        "nDCG@10": scores[f"{evaluator.name}_cosine_ndcg@10"],
        "MAP@100": scores[f"{evaluator.name}_cosine_map@100"],
    }

models_to_test = {
    "BASE (cosmos)": MODEL_NAME,
    "Model (seed 42)": os.path.join(PROJE_PATH, "model_seed_42"),
    "Model (seed 123)": os.path.join(PROJE_PATH, "model_seed_123"),
    "Model (seed 999)": os.path.join(PROJE_PATH, "model_seed_999"),
    "MERGED ENSEMBLE": os.path.join(PROJE_PATH, "cosmos_merged_final"),
    "UNION (3 subsets)": os.path.join(PROJE_PATH, "model_union_3subsets"),
}

rows = []
for name, path in models_to_test.items():
    print("\nðŸ”", name, "->", path)
    clear_mem()
    m = SentenceTransformer(path, device="cuda")

    with torch.no_grad():
        out = evaluator(m)

    scores = ensure_metrics_dict(out)
    rows.append({"Model": name, **pick(scores)})

    del m
    clear_mem()

df = pd.DataFrame(rows).sort_values("MRR@10", ascending=False)
print("\nâœ… RESULTS:")
print(df.to_string(index=False))

csv_path = os.path.join(PROJE_PATH, "results_same_evaluator_updated.csv")
df.to_csv(csv_path, index=False)
print("\nðŸ’¾ Saved:", csv_path)

